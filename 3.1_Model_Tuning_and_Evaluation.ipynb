{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "525aba37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 3.1: Model Selection, Cross-Validation & Tuning ---\n",
      "\n",
      "Comparing all models...\n",
      "\n",
      "--- Model Comparison Table ---\n",
      "               Model        R2       MAE      RMSE\n",
      "2  Gradient Boosting  0.529372  0.153693  0.213988\n",
      "1      Random Forest  0.494986  0.163305  0.221668\n",
      "0  Linear Regression  0.487000  0.160359  0.223414\n",
      "3     Neural Network  0.322185  0.204415  0.256807\n",
      "\n",
      "Tuning the winner: Gradient Boosting...\n",
      "\n",
      "--- FINAL TUNED MODEL PERFORMANCE ---\n",
      "Best Params: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50, 'random_state': 42}\n",
      "R2: 0.5363\n",
      "MAE: 0.1519\n",
      "RMSE: 0.2124\n",
      "\n",
      "Saved: final_model_tuned.pkl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def run_full_modeling_process():\n",
    "    print(\"--- Step 3.1: Model Selection, Cross-Validation & Tuning ---\")\n",
    "    \n",
    "    # 1. Load Data\n",
    "    df = pd.read_csv('final_features_pro.csv', index_col='dt', parse_dates=True)\n",
    "    X = df.drop(columns=['Global_active_power'])\n",
    "    y = df['Global_active_power']\n",
    "    \n",
    "    # 2. Split and Scale\n",
    "    split = int(len(df) * 0.8)\n",
    "    X_train_raw, X_test_raw = X.iloc[:split], X.iloc[split:]\n",
    "    y_train, y_test = y.iloc[:split], y.iloc[split:]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train_raw)\n",
    "    X_test = scaler.transform(X_test_raw)\n",
    "    joblib.dump(scaler, 'data_scaler.pkl')\n",
    "\n",
    "    # 3. Model Comparison (Requirement: \"Compare model performance\")\n",
    "    models = {\n",
    "        \"Linear Regression\": LinearRegression(),\n",
    "        \"Random Forest\": RandomForestRegressor(random_state=42),\n",
    "        \"Gradient Boosting\": GradientBoostingRegressor(random_state=42),\n",
    "        \"Neural Network\": MLPRegressor(max_iter=1000, random_state=42)\n",
    "    }\n",
    "\n",
    "    comparison_results = []\n",
    "    \n",
    "    print(\"\\nComparing all models...\")\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_test)\n",
    "        \n",
    "        comparison_results.append({\n",
    "            \"Model\": name,\n",
    "            \"R2\": r2_score(y_test, preds),\n",
    "            \"MAE\": mean_absolute_error(y_test, preds),\n",
    "            \"RMSE\": np.sqrt(mean_squared_error(y_test, preds))\n",
    "        })\n",
    "\n",
    "    # Display comparison\n",
    "    results_df = pd.DataFrame(comparison_results).sort_values(by='R2', ascending=False)\n",
    "    print(\"\\n--- Model Comparison Table ---\")\n",
    "    print(results_df)\n",
    "\n",
    "    # 4. Hyperparameter Tuning on the Winner (Requirement: \"Perform hyperparameter tuning\")\n",
    "    # We choose Gradient Boosting as it usually wins this dataset\n",
    "    print(f\"\\nTuning the winner: Gradient Boosting...\")\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=5) # Cross-validation\n",
    "\n",
    "    #Hyperparameter Tuning\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'max_depth': [3, 5],\n",
    "        'random_state': [42]\n",
    "    }\n",
    "    \n",
    "    grid_search = GridSearchCV(GradientBoostingRegressor(), param_grid, cv=tscv, scoring='r2', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    final_preds = best_model.predict(X_test)\n",
    "\n",
    "    # 5. Final Results for Best Model\n",
    "    print(\"\\n--- FINAL TUNED MODEL PERFORMANCE ---\")\n",
    "    print(f\"Best Params: {grid_search.best_params_}\")\n",
    "    print(f\"R2: {r2_score(y_test, final_preds):.4f}\")\n",
    "    print(f\"MAE: {mean_absolute_error(y_test, final_preds):.4f}\")\n",
    "    print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, final_preds)):.4f}\")\n",
    "\n",
    "    joblib.dump(best_model, 'final_model_tuned.pkl')\n",
    "    print(\"\\nSaved: final_model_tuned.pkl\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_full_modeling_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23a95f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015655b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16fcd8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2cc41d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d06c68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69718ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 3: Model Selection & Hyperparameter Tuning ---\n",
      "\n",
      "Evaluating baseline models...\n",
      "\n",
      "--- Baseline Comparison Table ---\n",
      "                  Model        R2       MAE      RMSE\n",
      "2     Gradient Boosting  0.529372  0.153693  0.213988\n",
      "1         Random Forest  0.494986  0.163305  0.221668\n",
      "0     Linear Regression  0.487000  0.160359  0.223414\n",
      "3  Neural Network (MLP)  0.322185  0.204415  0.256807\n",
      "\n",
      "Refining the Winner: Tuning Gradient Boosting...\n",
      "\n",
      "--- Final Tuned Model Performance ---\n",
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50, 'random_state': 42}\n",
      "Final R2: 0.5363\n",
      "Final MAE: 0.1519\n",
      "Final RMSE: 0.2124\n",
      "\n",
      "Success: Tuned model saved as 'final_model_tuned.pkl'\n"
     ]
    }
   ],
   "source": [
    "#other way to write above code\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "import joblib\n",
    "\n",
    "def run_pro_comparison_with_tuning(input_file):\n",
    "    print(f\"--- Step 3: Model Selection & Hyperparameter Tuning ---\")\n",
    "    \n",
    "    # 1. Load data\n",
    "    df = pd.read_csv(input_file, index_col='dt', parse_dates=True)\n",
    "    X = df.drop(columns=['Global_active_power'])\n",
    "    y = df['Global_active_power']\n",
    "    \n",
    "    # 2. Time-Series Split (Ensuring reproducibility)\n",
    "    split = int(len(df) * 0.8)\n",
    "    X_train_raw, X_test_raw = X.iloc[:split], X.iloc[split:]\n",
    "    y_train, y_test = y.iloc[:split], y.iloc[split:]\n",
    "\n",
    "    # 3. Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train_raw)\n",
    "    X_test = scaler.transform(X_test_raw)\n",
    "    joblib.dump(scaler, 'data_scaler.pkl')\n",
    "\n",
    "    # 4. Initial Model Comparison (Requirement: RMSE, MAE, R-Squared)\n",
    "    models = {\n",
    "        \"Linear Regression\": LinearRegression(),\n",
    "        \"Random Forest\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "        \"Gradient Boosting\": GradientBoostingRegressor(random_state=42),\n",
    "        \"Neural Network (MLP)\": MLPRegressor(max_iter=1000, random_state=42)\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "    print(\"\\nEvaluating baseline models...\")\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        predictions = model.predict(X_test)\n",
    "        \n",
    "        r2 = r2_score(y_test, predictions)\n",
    "        mae = mean_absolute_error(y_test, predictions)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "        \n",
    "        results.append({\"Model\": name, \"R2\": r2, \"MAE\": mae, \"RMSE\": rmse})\n",
    "\n",
    "    # Display initial results\n",
    "    results_df = pd.DataFrame(results).sort_values(by='R2', ascending=False)\n",
    "    print(\"\\n--- Baseline Comparison Table ---\")\n",
    "    print(results_df)\n",
    "\n",
    "    # 5. Hyperparameter Tuning (Requirement: \"Perform hyperparameter tuning\")\n",
    "    # We tune Gradient Boosting since it typically performs best on this data\n",
    "    print(f\"\\nRefining the Winner: Tuning Gradient Boosting...\")\n",
    "    \n",
    "    # Requirement: \"Validate models using cross-validation techniques\"\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'max_depth': [3, 5],\n",
    "        'random_state': [42]\n",
    "    }\n",
    "    \n",
    "    # Optimized search\n",
    "    grid_search = GridSearchCV(GradientBoostingRegressor(), param_grid, cv=tscv, scoring='r2', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    final_predictions = best_model.predict(X_test)\n",
    "\n",
    "    # 6. Final Results for Tuned Model\n",
    "    print(\"\\n--- Final Tuned Model Performance ---\")\n",
    "    print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Final R2: {r2_score(y_test, final_predictions):.4f}\")\n",
    "    print(f\"Final MAE: {mean_absolute_error(y_test, final_predictions):.4f}\")\n",
    "    print(f\"Final RMSE: {np.sqrt(mean_squared_error(y_test, final_predictions)):.4f}\")\n",
    "\n",
    "    # 7. Save the optimized model\n",
    "    joblib.dump(best_model, 'final_model_tuned.pkl')\n",
    "    print(f\"\\nSuccess: Tuned model saved as 'final_model_tuned.pkl'\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_pro_comparison_with_tuning('final_features_pro.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1bfb60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03666d47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19081578",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1798c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
